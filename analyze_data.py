#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ‹›è˜ä¿¡æ¯æ•°æ®åˆ†æå·¥å…·
ç”¨äºåˆ†æå’Œç­›é€‰çˆ¬å–çš„æ‹›è˜ä¿¡æ¯
"""

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
import re


class RecruitmentAnalyzer:
    """æ‹›è˜ä¿¡æ¯åˆ†æå™¨"""

    def __init__(self, data_dir="output/zhihu"):
        self.data_dir = data_dir
        self.contents = []
        self.comments = []

    def load_data(self):
        """åŠ è½½çˆ¬å–çš„æ•°æ®"""
        print("ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®...")

        # åŠ è½½å¸–å­æ•°æ®
        search_dir = os.path.join(self.data_dir, "search")
        if os.path.exists(search_dir):
            for filename in os.listdir(search_dir):
                if filename.endswith(".json"):
                    filepath = os.path.join(search_dir, filename)
                    with open(filepath, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.contents.extend(data)

        # åŠ è½½è¯„è®ºæ•°æ®
        comments_dir = os.path.join(self.data_dir, "comments")
        if os.path.exists(comments_dir):
            for filename in os.listdir(comments_dir):
                if filename.endswith(".json"):
                    filepath = os.path.join(comments_dir, filename)
                    with open(filepath, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        if isinstance(data, list):
                            self.comments.extend(data)

        print(f"âœ“ å·²åŠ è½½ {len(self.contents)} æ¡å¸–å­")
        print(f"âœ“ å·²åŠ è½½ {len(self.comments)} æ¡è¯„è®º")

    def analyze_quality(self):
        """åˆ†ææ•°æ®è´¨é‡"""
        print("\n" + "="*60)
        print("ğŸ“Š æ•°æ®è´¨é‡åˆ†æ")
        print("="*60)

        if not self.contents:
            print("âš ï¸  æš‚æ— æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œçˆ¬è™«")
            return

        # ç»Ÿè®¡çƒ­åº¦åˆ†å¸ƒ
        high_quality = []  # é«˜è´¨é‡
        medium_quality = []  # ä¸­ç­‰è´¨é‡
        low_quality = []  # ä½è´¨é‡

        for item in self.contents:
            voteup = item.get("voteup_count", 0)
            comments = item.get("comment_count", 0)

            if voteup >= 20 or comments >= 10:
                high_quality.append(item)
            elif voteup >= 5 or comments >= 3:
                medium_quality.append(item)
            else:
                low_quality.append(item)

        print(f"\nğŸ“ˆ è´¨é‡åˆ†å¸ƒ:")
        print(f"  â­ é«˜è´¨é‡ (ç‚¹èµâ‰¥20 æˆ– è¯„è®ºâ‰¥10): {len(high_quality)} æ¡")
        print(f"  â­ ä¸­ç­‰è´¨é‡ (ç‚¹èµâ‰¥5 æˆ– è¯„è®ºâ‰¥3): {len(medium_quality)} æ¡")
        print(f"  â­ ä½è´¨é‡: {len(low_quality)} æ¡")

        return high_quality, medium_quality, low_quality

    def filter_by_keywords(self, keywords=None, min_quality=5):
        """æ ¹æ®å…³é”®è¯å’Œè´¨é‡ç­›é€‰"""
        print("\n" + "="*60)
        print("ğŸ” ç­›é€‰æ‹›è˜ä¿¡æ¯")
        print("="*60)

        if keywords is None:
            keywords = ["æ‹›è˜", "å†…æ¨", "æ ¡æ‹›", "å®ä¹ ", "å·¥ç¨‹å¸ˆ", "å¼€å‘"]

        results = []
        for item in self.contents:
            # æ£€æŸ¥å…³é”®è¯
            title = item.get("title", "")
            content = item.get("content_text", "")

            keyword_found = any(kw in title or kw in content for kw in keywords)

            # æ£€æŸ¥è´¨é‡
            voteup = item.get("voteup_count", 0)
            quality_ok = voteup >= min_quality

            if keyword_found and quality_ok:
                results.append(item)

        print(f"\nâœ“ ç­›é€‰æ¡ä»¶:")
        print(f"  â€¢ å…³é”®è¯: {', '.join(keywords[:5])}")
        print(f"  â€¢ æœ€ä½ç‚¹èµæ•°: {min_quality}")
        print(f"\nâœ“ æ‰¾åˆ° {len(results)} æ¡ç¬¦åˆæ¡ä»¶çš„æ‹›è˜ä¿¡æ¯")

        return results

    def extract_companies(self):
        """æå–å…¬å¸ä¿¡æ¯"""
        print("\n" + "="*60)
        print("ğŸ¢ å…¬å¸ä¿¡æ¯ç»Ÿè®¡")
        print("="*60)

        # å¸¸è§å…¬å¸å
        company_keywords = [
            "å­—èŠ‚è·³åŠ¨", "å­—èŠ‚", "æŠ–éŸ³", "TikTok",
            "è…¾è®¯", "å¾®ä¿¡",
            "é˜¿é‡Œ", "é˜¿é‡Œå·´å·´", "æ·˜å®", "æ”¯ä»˜å®", "èš‚èš",
            "ç¾å›¢", "ç¾å›¢ç‚¹è¯„",
            "äº¬ä¸œ",
            "ç™¾åº¦",
            "åä¸º",
            "å°ç±³",
            "ç½‘æ˜“",
            "æ»´æ»´",
            "å¿«æ‰‹",
            "Bç«™", "å“”å“©å“”å“©",
            "æ‹¼å¤šå¤š",
            "æºç¨‹",
            "äº¬ä¸œ",
        ]

        company_stats = defaultdict(int)
        company_posts = defaultdict(list)

        for item in self.contents:
            text = item.get("title", "") + " " + item.get("content_text", "")
            for company in company_keywords:
                if company in text:
                    company_stats[company] += 1
                    company_posts[company].append(item)

        # æ’åº
        sorted_companies = sorted(company_stats.items(), key=lambda x: x[1], reverse=True)

        print(f"\nğŸ“Š æ‹›è˜ä¿¡æ¯Top 10å…¬å¸:")
        for i, (company, count) in enumerate(sorted_companies[:10], 1):
            print(f"  {i:2d}. {company:12s} - {count:3d} æ¡")

        return dict(company_posts)

    def show_top_posts(self, posts=None, n=10):
        """å±•ç¤ºçƒ­é—¨å¸–å­"""
        print("\n" + "="*60)
        print(f"ğŸ”¥ çƒ­é—¨æ‹›è˜å¸– Top {n}")
        print("="*60)

        if posts is None:
            posts = self.contents

        # æŒ‰çƒ­åº¦æ’åº
        sorted_posts = sorted(
            posts,
            key=lambda x: x.get("voteup_count", 0),
            reverse=True
        )[:n]

        for i, post in enumerate(sorted_posts, 1):
            title = post.get("title", "")[:50]
            voteup = post.get("voteup_count", 0)
            comments = post.get("comment_count", 0)
            author = post.get("user_nickname", "æœªçŸ¥")
            url = post.get("content_url", "")

            print(f"\n{i}. {title}")
            print(f"   ğŸ‘ {voteup} ç‚¹èµ  |  ğŸ’¬ {comments} è¯„è®º  |  ğŸ‘¤ {author}")
            print(f"   ğŸ”— {url[:80]}")

    def show_recent_posts(self, days=7, n=10):
        """å±•ç¤ºæœ€è¿‘çš„å¸–å­"""
        print("\n" + "="*60)
        print(f"ğŸ“… æœ€è¿‘ {days} å¤©çš„æ‹›è˜ä¿¡æ¯ (Top {n})")
        print("="*60)

        now = int(datetime.now().timestamp())
        cutoff = now - (days * 86400)

        recent_posts = []
        for item in self.contents:
            created_time = item.get("created_time", 0)
            if created_time >= cutoff:
                recent_posts.append(item)

        # æŒ‰æ—¶é—´æ’åº
        recent_posts.sort(key=lambda x: x.get("created_time", 0), reverse=True)

        print(f"\nâœ“ æ‰¾åˆ° {len(recent_posts)} æ¡æœ€è¿‘ {days} å¤©çš„å¸–å­")

        for i, post in enumerate(recent_posts[:n], 1):
            title = post.get("title", "")[:50]
            created = post.get("created_time", 0)
            if created:
                date_str = datetime.fromtimestamp(created).strftime("%Y-%m-%d %H:%M")
            else:
                date_str = "æœªçŸ¥"
            voteup = post.get("voteup_count", 0)

            print(f"\n{i}. {title}")
            print(f"   ğŸ“… {date_str}  |  ğŸ‘ {voteup} ç‚¹èµ")

    def export_filtered_data(self, posts, filename="filtered_recruitment.json"):
        """å¯¼å‡ºç­›é€‰åçš„æ•°æ®"""
        output_path = os.path.join(self.data_dir, filename)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(posts, f, ensure_ascii=False, indent=2)

        print(f"\nâœ“ å·²å¯¼å‡º {len(posts)} æ¡æ•°æ®åˆ° {output_path}")

    def generate_report(self):
        """ç”Ÿæˆå®Œæ•´åˆ†ææŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“‹ æ‹›è˜ä¿¡æ¯åˆ†ææŠ¥å‘Š")
        print("="*60)
        print(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)

        # 1. è´¨é‡åˆ†æ
        high, medium, low = self.analyze_quality()

        # 2. å…³é”®è¯ç­›é€‰
        filtered = self.filter_by_keywords(min_quality=5)

        # 3. å…¬å¸ç»Ÿè®¡
        company_posts = self.extract_companies()

        # 4. çƒ­é—¨å¸–å­
        self.show_top_posts(filtered[:20])

        # 5. æœ€è¿‘å¸–å­
        self.show_recent_posts(days=7, n=10)

        # 6. å¯¼å‡ºé«˜è´¨é‡æ•°æ®
        if high:
            self.export_filtered_data(high, "high_quality_recruitment.json")

        print("\n" + "="*60)
        print("âœ“ åˆ†æå®Œæˆï¼")
        print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘          æ‹›è˜ä¿¡æ¯æ•°æ®åˆ†æå·¥å…· v1.0                      â•‘
â•‘          Recruitment Data Analyzer                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    # æ£€æŸ¥æ•°æ®ç›®å½•
    data_dir = "output/zhihu"
    if not os.path.exists(data_dir):
        print(f"âš ï¸  æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print("\nğŸ’¡ è¯·å…ˆè¿è¡Œçˆ¬è™«é‡‡é›†æ•°æ®:")
        print("   ./run_crawler.sh")
        print("   æˆ–")
        print("   python main.py --platform zhihu --lt cookie --type search")
        return

    # åˆ›å»ºåˆ†æå™¨
    analyzer = RecruitmentAnalyzer(data_dir)

    # åŠ è½½æ•°æ®
    analyzer.load_data()

    # ç”ŸæˆæŠ¥å‘Š
    if analyzer.contents:
        analyzer.generate_report()
    else:
        print("âš ï¸  æœªæ‰¾åˆ°æ•°æ®ï¼Œè¯·æ£€æŸ¥çˆ¬è™«æ˜¯å¦æ­£å¸¸è¿è¡Œ")


if __name__ == "__main__":
    main()
